{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informacje biznesowe\n",
    "\n",
    "Kontakt z autorem: [adam_lewandowski_1998@outlook.com](mailto:adam_lewandowski_1998@outlook.com)\n",
    "\n",
    "Celem byłoby pobranie, poprzez webscrapping (?), kategorii dla przepisów. Wynik mógłby być w postaci np. pliku csv zawierającego wiersze z kolumnami:\n",
    "1) link (np. https://cookbooks.com/Recipe-Details.aspx?id=434612), który uznajemy za identyfikator przepisu, plus\n",
    "2) lista kategorii, np. [‘Healthy’, ‘Salad’, ‘Quick and Easy’]\n",
    "Np. na stronie cookbooks.com mamy po lewej stronie menu z podziałem przepisów na kategorie. Jeśli, przykładowo, klikniemy w Healthy, to dostaniemy listę przepisów, które opisane są taką kategorią.\n",
    "Pewne utrudnienie: jeśli kliknę w link do takiego przepisu, to (przynajmniej na pierwszy rzut oka) nie mam nigdzie takiego tagu/kategorii. Pewno więc trzeba by było robić odwrotnie: pobierać zbiór przepisów pod daną kategorię i dopiero w drugim kroku zrobić z tego listę linków ze zbiorami kategorii.\n",
    "Linki miałyby pochodzić jednocześnie np. ze strony Cookbooks.com, ale wyselekcjonowane te, które już są we zbiorze RecipeNLG, bo ten zbiór chcemy wzbogacać. Pewno więc można by zacząć od stworzenia listy linków ze zbioru RecipeNLG, które pochodzą z Cookbooks.com.\n",
    "Na początek interesująca jest strona Cookbooks.com, ponieważ najwięcej przepisów w zbiorze RecipeNLG pochodzi z tej strony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import re\n",
    "from itables import init_notebook_mode\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "init_notebook_mode(all_interactive=True)\n",
    "BASE_URL = \"https://cookbooks.com/\"\n",
    "LINK_TAG = \"a\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nawigacja po stronie wymaga inicjalizacji sesji ze względu na zapisywanie ciasteczek w przestrzeni przeglądarki użytkownika.\n",
    "Ekstrakcja przepisów wymaga podróżowania bezpośrednio do podkategorii ze względu na wykorzystywanie poprzedniego odwiedzonego adresu do ustalania następnego uwzględniając daną kategorię (pod-strony kategorii współdzielą ten sam adres URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "\n",
    "def get_category_recipies(category_url: str) -> list[list[dict]]:\n",
    "    \"\"\"\n",
    "    Extract category recipies from subpages.\n",
    "\n",
    "    Args:\n",
    "        category_url (str): category sub-page resource address\n",
    "\n",
    "    Returns:\n",
    "        list[list[dict]]: category recipies ids with titles\n",
    "    \"\"\"\n",
    "    TO_REMOVE = re.compile(r\"\\d+\\. \")\n",
    "\n",
    "    recipies = []\n",
    "    text = session.get(category_url).content\n",
    "\n",
    "    html = BeautifulSoup(text, \"html.parser\")\n",
    "    links = list(html.find_all(\"a\"))\n",
    "\n",
    "    recipies.extend(\n",
    "        [\n",
    "            {\n",
    "                \"href\": f\"{BASE_URL}{a['href']}\",\n",
    "                \"object\": TO_REMOVE.sub(\"\", a.text),\n",
    "            }\n",
    "            for a in links\n",
    "            # url to object\n",
    "            if a[\"href\"].startswith(\"Recipe-Details.aspx?\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return recipies\n",
    "\n",
    "\n",
    "def paginate_category_pages(category_url: str):\n",
    "    \"\"\"\n",
    "    Generator to retrieve category subpages urls.\n",
    "\n",
    "    Args:\n",
    "        category_url (str): category resource address of recipies\n",
    "\n",
    "    Yields:\n",
    "        str: category sub-page resources to visit\n",
    "    \"\"\"\n",
    "    text = session.get(category_url).content\n",
    "\n",
    "    html = BeautifulSoup(text, \"html.parser\")\n",
    "    links = list(html.find_all(\"a\"))\n",
    "\n",
    "    for a in links:\n",
    "        if a[\"href\"].startswith(\"recipes-search.aspx?page\"):\n",
    "            page_url: str = f\"{BASE_URL}{a['href']}\"\n",
    "            yield page_url\n",
    "\n",
    "\n",
    "def get_all_categories() -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Fetch all categories from page menu, then visits each of them in menu\n",
    "    order and using page navigation gets links to recipies.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list[str]]: categories with assigned recipies id's in url \n",
    "        format\n",
    "    \"\"\"\n",
    "    base_page_html = BeautifulSoup(\n",
    "        requests.get(BASE_URL).content, \"html.parser\"\n",
    "    )\n",
    "    menu = typing.cast(\n",
    "        Tag, base_page_html.find(\"div\", attrs={\"class\": \"arrowgreen\"})\n",
    "    )\n",
    "\n",
    "    recipies = defaultdict(list)\n",
    "    categories = list(menu.find_all(LINK_TAG))\n",
    "    for tag in tqdm(categories):\n",
    "        category = tag.text\n",
    "        first_page_url = f'{BASE_URL}{tag[\"href\"][3:]}'\n",
    "\n",
    "        # we have to visit all the pages in a category before moving\n",
    "        # to the next category\n",
    "        for url in chain(\n",
    "            [first_page_url], paginate_category_pages(first_page_url)\n",
    "        ):\n",
    "            result = get_category_recipies(url)\n",
    "            recipies[category].extend(result)\n",
    "\n",
    "    return recipies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipies = get_all_categories()\n",
    "recipies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {**el, **{cat: int(el in recipies[cat]) for cat in recipies.keys()}}\n",
    "    for rec in recipies.values()\n",
    "    for el in rec\n",
    "]\n",
    "df = pd.DataFrame(data).drop_duplicates()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../out/results_unnormalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posprocessing dla klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = (\n",
    "    df.drop(columns=[\"href\"])\n",
    "    .groupby(\"object\")\n",
    "    .sum(numeric_only=True)\n",
    "    .applymap(lambda x: min(x, 1))\n",
    ")\n",
    "\n",
    "classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification.to_csv(\"../out/results_classification.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d71527e0dd44f795681ff70e41af4afeb0c5f4d47a79545f42bdef8e803f147b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
